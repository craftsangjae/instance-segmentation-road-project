{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from engine.parallel import ParallelModel\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from engine.config import ModelConfiguration\n",
    "from engine.utils import *\n",
    "\n",
    "for dirpath in glob.glob(\"../logs/*\"):\n",
    "    if os.path.isdir(dirpath) and len(os.listdir(dirpath))==0:\n",
    "        os.removedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ Data Generator \\]\n",
    "---\n",
    "\n",
    "> 우리는 Semantic Segmentation & Instance Segmentation을 수행하는 모델을 개발."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.retinamasklab import construct_masklabdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary\n",
      "-------------------------------\n",
      "* Num of Train Images : 0\n",
      "* Num of Valid Images : 0\n",
      "* Num of Images : 0\n",
      "-------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfiguration()\n",
    "config.dataset.data_dir = \"../road_project/datasets/\"\n",
    "trainset, validset = construct_masklabdataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ Allocate Trainable(Sharing) layers \\]\n",
    "---\n",
    "\n",
    "> 학습할 때와 추론할 때, 모델은 서로 다르게 동작함. 먼저 학습할 Weight Layer들을 구성 후, Training Model과 Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.retinamasklab import build_backbone_network\n",
    "from engine.retinamasklab import build_detection_network\n",
    "from engine.retinamasklab import build_semantic_network\n",
    "from engine.retinamasklab import build_instance_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/seresnet34_imagenet_1000_no_top.h5\n",
      "86319104/86315168 [==============================] - 41s 0us/step\n",
      "Backbone Network Summary\n",
      "------------------------\n",
      "* Backbone Type : seresnet34\n",
      "* Backbone outputs : ('C3', 'C4', 'C5', 'P6')\n",
      "* Num features of Backbone Additional Layers: 128\n",
      "------------------------\n",
      "\n",
      "Prior Network Summary\n",
      "-------------------------------\n",
      "* Strides of prior : [8, 16, 32, 64]\n",
      "* Sizes of prior : [32, 64, 128, 256]\n",
      "* width/height scales of prior : [1, 1.2599210498948732, 1.5874010519681994]\n",
      "* width/height ratios of prior : [0.3333333333333333, 0.5, 1, 2, 3]\n",
      "-------------------------------\n",
      "\n",
      "Feature Pyramid Network Summary\n",
      "-------------------------------\n",
      "* Feature Pyramid Inputs  : ('C3', 'C4', 'C5')\n",
      "* Num Features of Feature Pyramid : 128\n",
      "-------------------------------\n",
      "\n",
      "Detection Head Networks Summary\n",
      "-------------------------------\n",
      "* Num Classes of Detection Classes : 5\n",
      "* Num Depth of Sub Networks : 3\n",
      "* Num Features of Sub Networks : 128\n",
      "* Use Separable Conv : False\n",
      "* Use Squeeze Excite : True\n",
      "* squeeze ratio in Squeeze Excite: 16\n",
      "* Num Groups of Group Normalization : 16\n",
      "-------------------------------\n",
      "\n",
      "MaskDistribute Network Summary\n",
      "-------------------------------\n",
      "* max_k      : 2\n",
      "* base_size  : 36\n",
      "-------------------------------\n",
      "\n",
      "Pyramid Roi Align Network Summary\n",
      "-------------------------------\n",
      "* Crop size : (14, 14)\n",
      "-------------------------------\n",
      "\n",
      "Instance Head Network Summary\n",
      "-------------------------------\n",
      "* Num Classes of Instance Classes : 5\n",
      "* Num Depth of Head Network : 3\n",
      "* Num Features of Head Network : 128\n",
      "* Use Separable Conv : False\n",
      "* Use Squeeze Excite : True\n",
      "* squeeze ratio in Squeeze Excite: 16\n",
      "* Num Groups of Group Normalization : 16\n",
      "-------------------------------\n",
      "\n",
      "Atrous Spatial Pyramid Pooling Network Summary\n",
      "-------------------------------\n",
      "* num features : 128\n",
      "* Atrous Rates : (6, 12, 18)\n",
      "* groups : 16\n",
      "-------------------------------\n",
      "\n",
      "Semantic Head Networks Summary\n",
      "-------------------------------\n",
      "* Num Classes of Semantic Classes : 3\n",
      "* Num Depth of Head Network : 3\n",
      "* Num Features of Head Network : 128\n",
      "* Num Skip Features of Head Network : 32\n",
      "* Use Separable Conv : False\n",
      "* Use Squeeze Excite : True\n",
      "* squeeze ratio in Squeeze Excite: 16\n",
      "* Num Groups of Group Normalization : 16\n",
      "-------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "#####################\n",
    "# Allocate Trainable Weights\n",
    "#####################\n",
    "\n",
    "## 1. Build Base Network\n",
    "config.backbone.backbone_type = 'seresnet34'\n",
    "config.backbone.backbone_outputs = ('C3', 'C4', 'C5', 'P6')\n",
    "\n",
    "backbone_network = build_backbone_network(config)\n",
    "\n",
    "## 2. Build Layers Related On Detection\n",
    "config.detection.num_features = 128\n",
    "config.detection.num_depth = 3\n",
    "config.detection.use_squeeze_excite = True\n",
    "\n",
    "detection_networks = build_detection_network(config)\n",
    "\n",
    "## 4. Build Layers Related On Mask Prediction\n",
    "config.instance.use_squeeze_excite = True\n",
    "config.instance.num_features = 128\n",
    "config.instance.num_depth = 3\n",
    "\n",
    "instance_networks = build_instance_network(config)\n",
    "\n",
    "## 3. Build Layers Related On Semantic Segmentation\n",
    "config.semantic.atrous_groups = 16\n",
    "config.semantic.num_features = 128\n",
    "config.semantic.num_depth = 3\n",
    "config.semantic.use_separable_conv = False\n",
    "config.semantic.use_squeeze_excite = True\n",
    "\n",
    "semantic_networks = build_semantic_network(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ Build Trainer & Predictor \\]\n",
    "---\n",
    "\n",
    "> Weight들을 공유하는 Trainer와 Predictor을 각각 선언."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.retinamasklab import construct_trainer_network\n",
    "from engine.retinamasklab import construct_inference_network\n",
    "from engine.losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Losses Summary\n",
      "-------------------------------\n",
      "* Classification Loss Weight: 300\n",
      "* Classification Loss alpha: 0.25\n",
      "* Classification Loss gamma: 2.0\n",
      "* Localization Loss Weight: 1.0\n",
      "* Localization Use adjusted Smoothing L1: True\n",
      "* Localization Loss momentum: 0.9\n",
      "* Localization Loss beta: 0.11\n",
      "-------------------------------\n",
      "\n",
      "Instance Loss Summary\n",
      "-------------------------------\n",
      "* Instance Loss Weight: 0.01\n",
      "* Instance Loss Label Smoothing: 0.0\n",
      "-------------------------------\n",
      "\n",
      "Semantic Loss Summary\n",
      "-------------------------------\n",
      "* Semantic Loss Weight: 0.5\n",
      "* Semantic Loss Label Smoothing: 0.0\n",
      "-------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = construct_trainer_network(\n",
    "    configuration=config, backbone_network=backbone_network,\n",
    "    detection_networks=detection_networks,\n",
    "    semantic_networks=semantic_networks,\n",
    "    instance_networks=instance_networks)\n",
    "\n",
    "inference = construct_inference_network(\n",
    "    configuration=config, backbone_network=backbone_network,\n",
    "    detection_networks=detection_networks,\n",
    "    semantic_networks=semantic_networks,\n",
    "    instance_networks=instance_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ train Multi-GPU Trainer \\]\n",
    "---\n",
    "\n",
    "> ParallelModel을 구성함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 학습 준비하기 \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Set-Up 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Training Set-Up\n",
    "#####################\n",
    "\n",
    "## 1. Batch size\n",
    "batch_size = config.train.batch_size\n",
    "\n",
    "## 2. GPU Count\n",
    "gpu_count = config.train.gpu_count\n",
    "\n",
    "## 3. Save Directory Set-Up\n",
    "dt = datetime.strftime(datetime.now(),'%m-%d-%H')\n",
    "save_dir = f\"../logs/without_naver/{config.backbone.backbone_type}/{dt}/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "config.train.save_dir = save_dir\n",
    "\n",
    "## 4. Save Configuration File\n",
    "with open(os.path.join(save_dir, 'config.json'),'w') as f:\n",
    "    json.dump(config.to_dict(),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) CallBack 함수 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.callbacks import SaveInferenceModel, CyclicLR\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = SaveInferenceModel(save_dir, inference)\n",
    "tb = TensorBoard(save_dir)\n",
    "csvl = CSVLogger(\n",
    "    os.path.join(save_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) DataGenerator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = MaskLabGenerator(trainset.config,\n",
    "                             scale_ratio=(.4,.6),\n",
    "                             batch_size=batch_size)\n",
    "\n",
    "validgen = MaskLabGenerator(validset.config,\n",
    "                             scale_ratio=.5, \n",
    "                             batch_size=32,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.parallel import ParallelModel\n",
    "from engine.optimizers import RectifiedAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) 모델 학습 가능한지 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Fit Dataset Successfully\n",
      "10/10 [==============================] - 594s 59s/step - loss: 0.7495 - class_loss: 0.1244 - box_loss: 0.3048 - detection_precision_metric: 0.0000e+00 - detection_recall_metric: 0.0000e+00 - detection_fmeasure_metric: 0.0000e+00 - mask_loss: 0.0045 - seg_loss: 0.3159 - other_road_iou_metric: 0.3921 - my_road_metric: 0.2171 - crack_iou_metric: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb6ba2f550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Check Fit Dataset Successfully\")\n",
    "checkgen = MaskLabGenerator(trainset, \n",
    "                            scale_ratio=.6, \n",
    "                            batch_size=batch_size,)\n",
    "# Full Trainable\n",
    "for layer in trainer.layers:\n",
    "    layer.trainable = True\n",
    "if gpu_count > 1:\n",
    "    parallel = ParallelModel(trainer, gpu_count)\n",
    "else:\n",
    "    parallel = trainer\n",
    "    \n",
    "# Set Loss & Metric\n",
    "for tensor, name in zip(parallel.output, parallel.output_names):\n",
    "    if 'loss' in name:\n",
    "        parallel.add_loss(K.mean(tensor))\n",
    "    parallel.add_metric(tensor, aggregation='mean',name=name)\n",
    "parallel.compile(RectifiedAdam(1e-10))\n",
    "\n",
    "parallel.fit_generator(checkgen, steps_per_epoch=10, verbose=1, \n",
    "                       callbacks=[tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Train Head Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BackBone Network\n",
    "freeze('C5')\n",
    "if gpu_count > 1:\n",
    "    parallel = ParallelModel(trainer, gpu_count)\n",
    "else:\n",
    "    parallel = trainer\n",
    "    \n",
    "# Set Loss & Metric\n",
    "for tensor, name in zip(parallel.output, parallel.output_names):\n",
    "    if 'loss' in name:\n",
    "        parallel.add_loss(K.mean(tensor))\n",
    "    parallel.add_metric(tensor, aggregation='mean',name=name)\n",
    "\n",
    "# Compile Model\n",
    "clr = CyclicLR(base_lr=1e-4, max_lr=1e-3, step_size=700)\n",
    "parallel.compile(RectifiedAdam(1e-3))\n",
    "callbacks = [ckpt, tb, clr, csvl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR Range Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save Current Weight\")\n",
    "parallel.save_weights(\"temp.h5\") # Weight 임시 저장\n",
    "\n",
    "lr_list = np.logspace(-7,-1,num=50)\n",
    "lrschedule = LearningRateScheduler(lambda x: lr_list[x],verbose=1)\n",
    "\n",
    "print(\"Start to do LR range Test\")\n",
    "parallel.compile(RectifiedAdam(1e-3))\n",
    "hist = parallel.fit_generator(traingen, \n",
    "                              steps_per_epoch=3, \n",
    "                              epochs=len(lr_list),\n",
    "                              verbose=1,\n",
    "                              callbacks=[lrschedule])\n",
    "\n",
    "print(\"Reload weight\")\n",
    "parallel.load_weights('temp.h5') # Weight 불러오기\n",
    "os.remove('temp.h5')\n",
    "\n",
    "plt.title(\"Learning Rate Range Test\")\n",
    "plt.plot(lr_list, hist.history['loss'])\n",
    "plt.xscale('log')\n",
    "plt.ylim((0.,2.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "clr = CyclicLR(base_lr=1e-4, max_lr=1e-3, step_size=700)\n",
    "parallel.compile(RectifiedAdam(1e-4))\n",
    "callbacks = [ckpt, tb, clr, csvl]\n",
    "\n",
    "parallel.fit_generator(traingen, epochs=10, steps_per_epoch=300,  \n",
    "                       validation_data=validgen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train FineTune Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BackBone Network\n",
    "freeze('C2')\n",
    "if gpu_count > 1:\n",
    "    parallel = ParallelModel(trainer, gpu_count)\n",
    "else:\n",
    "    parallel = trainer\n",
    "    \n",
    "# Set Loss & Metric\n",
    "for tensor, name in zip(parallel.output, parallel.output_names):\n",
    "    if 'loss' in name:\n",
    "        parallel.add_loss(K.mean(tensor))\n",
    "    parallel.add_metric(tensor, aggregation='mean',name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR Range Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save Current Weight\")\n",
    "parallel.save_weights(\"temp.h5\") # Weight 임시 저장\n",
    "\n",
    "lr_list = np.logspace(-8,-1,num=200)\n",
    "lrschedule = LearningRateScheduler(lambda x: lr_list[x],verbose=1)\n",
    "\n",
    "print(\"Start to do LR range Test\")\n",
    "parallel.compile(RectifiedAdam(1e-3))\n",
    "hist = parallel.fit_generator(traingen, \n",
    "                              steps_per_epoch=3, \n",
    "                              epochs=len(lr_list),\n",
    "                              verbose=1,\n",
    "                              callbacks=[lrschedule])\n",
    "\n",
    "print(\"Reload weight\")\n",
    "parallel.load_weights('temp.h5') # Weight 불러오기\n",
    "os.remove('temp.h5')\n",
    "\n",
    "plt.title(\"Learning Rate Range Test\")\n",
    "plt.plot(lr_list, hist.history['loss'])\n",
    "plt.xscale('log')\n",
    "plt.ylim((0.,1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "clr = CyclicLR(base_lr=1e-4, max_lr=1e-3, step_size=700)\n",
    "parallel.compile(RectifiedAdam(1e-3))\n",
    "callbacks = [ckpt, tb, clr, csvl]\n",
    "\n",
    "# Warm-Up \n",
    "parallel.fit_generator(traingen, epochs=20,\n",
    "                       steps_per_epoch=300, initial_epoch=10,\n",
    "                       validation_data=validgen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Train All Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BackBone Network\n",
    "for layer in trainer.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "if gpu_count > 1:\n",
    "    parallel = ParallelModel(trainer, gpu_count)\n",
    "else:\n",
    "    parallel = trainer\n",
    "    \n",
    "# Set Loss & Metric\n",
    "for tensor, name in zip(parallel.output, parallel.output_names):\n",
    "    if 'loss' in name:\n",
    "        parallel.add_loss(K.mean(tensor))\n",
    "    parallel.add_metric(tensor, aggregation='mean',name=name)\n",
    "\n",
    "# Compile Model\n",
    "clr = CyclicLR(base_lr=1e-5, max_lr=1e-3, step_size=700)\n",
    "parallel.compile(RectifiedAdam(1e-4))\n",
    "callbacks = [ckpt, tb, clr, csvl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm-Up \n",
    "parallel.fit_generator(traingen, epochs=70,\n",
    "                       steps_per_epoch=300, initial_epoch=20,\n",
    "                       validation_data=validgen, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
